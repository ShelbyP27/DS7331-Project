{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baab44a",
   "metadata": {},
   "source": [
    "# DS7331 Project 2\n",
    "#### Group 2: Hollie Gardner, Cleveland Johnson, Shelby Provost\n",
    "[Dataset Source](https://archive-beta.ics.uci.edu/ml/datasets/census+income)<br/>\n",
    "[Github Repo](https://github.com/ShelbyP27/DS7331-Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329f550a",
   "metadata": {},
   "outputs": [
    {
     "ename": "XGBoostError",
     "evalue": "XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-939bffcec85d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#exceptional work\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlazypredict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSupervised\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLazyClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLazyRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/lazypredict/Supervised.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m )\n\u001b[1;32m     27\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# import catboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeviceQuantileDMatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBooster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrabit\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;31m# load the XGBoost library globally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m \u001b[0m_LIB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_load_lib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_load_lib\u001b[0;34m()\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlib_success\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mlibname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlib_paths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         raise XGBoostError(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0;34m'XGBoost Library ({}) could not be loaded.\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;34m'Likely causes:\\n'\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: XGBoost Library (libxgboost.dylib) could not be loaded.\nLikely causes:\n  * OpenMP runtime is not installed (vcomp140.dll or libgomp-1.dll for Windows, libomp.dylib for Mac OSX, libgomp.so for Linux and other UNIX-like OSes). Mac OSX users: Run `brew install libomp` to install OpenMP runtime.\n  * You are running 32-bit Python on a 64-bit OS\nError message(s): ['dlopen(/opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib, 6): Library not loaded: /usr/local/opt/libomp/lib/libomp.dylib\\n  Referenced from: /opt/anaconda3/lib/python3.8/site-packages/xgboost/lib/libxgboost.dylib\\n  Reason: image not found']\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "import lazypredict\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#prediction models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#exceptional work\n",
    "from lazypredict.Supervised import LazyClassifier, LazyRegressor\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481c964",
   "metadata": {},
   "source": [
    "## Data Preparation: Part 1\n",
    "*Define and prepare your class variables. Use proper variable representations (int, float, one-hot, etc.). Use pre-processing methods (as needed) for dimensionality reduction, scaling, etc. Remove variables that are not needed/useful for the analysis.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa6180",
   "metadata": {},
   "source": [
    "### Loading and Prepping Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f225805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the census dataset using pandas\n",
    "# Reading the CSV file after converting file to csv and removing superfluous spaces via Excel.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ShelbyP27/DS7331-Project/main/adult-data.csv')\n",
    "\n",
    "# Getting a first look at the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up data set\n",
    "df = df.replace(to_replace='?',value=np.nan) # replace '?' with NaN (not a number)\n",
    "df.dropna(inplace=True) # Removing na values\n",
    "df.duplicated(subset=None, keep='first') #Remove duplicates\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int) #One-hot respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encoding the Categorical variables\n",
    "if 'sex' in df:\n",
    "    df['IsMale'] = df.sex == 'Male'\n",
    "    df.IsMale = df.IsMale.astype(np.int64)\n",
    "    del df['sex']\n",
    "    \n",
    "if 'marital-status' in df:\n",
    "    tmp_df = pd.get_dummies(df['marital-status'], prefix = 'Marital')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['marital-status']\n",
    "    \n",
    "if'relationship' in df:\n",
    "    tmp_df = pd.get_dummies(df['relationship'], prefix = 'Rel')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['relationship']\n",
    "\n",
    "if 'race' in df:\n",
    "    tmp_df = pd.get_dummies(df['race'], prefix = 'Race')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['race']\n",
    "\n",
    "if 'workclass' in df:\n",
    "    tmp_df = pd.get_dummies(df['workclass'], prefix = 'Work')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['workclass']\n",
    "\n",
    "if 'occupation' in df:\n",
    "    tmp_df = pd.get_dummies(df['occupation'], prefix = 'Occupation')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['occupation']\n",
    "\n",
    "if 'education' in df:\n",
    "    tmp_df = pd.get_dummies(df['education'], prefix = 'Education')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['education']\n",
    "\n",
    "    \n",
    "#Replace Native Country with Immigrant atribute\n",
    "if 'native-country' in df:\n",
    "    df['immigrant'] = np.where(df['native-country']!= 'United-States', 1, 0)\n",
    "    del df['native-country']\n",
    "df.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eda3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the response\n",
    "if 'income' in df:\n",
    "    y = df['income'].values\n",
    "    del df['income']\n",
    "    X = df.values\n",
    "\n",
    "# Train / Test split with scaled_X\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_X, y, test_size = .2, random_state=1)\n",
    "\n",
    "# Train / Test split\n",
    "#sc = StandardScaler()\n",
    "#sc.fit(X)\n",
    "#x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state=1)\n",
    "\n",
    "# scaledX = sc.fit_transform(X)\n",
    "# s_x_train, s_x_test, s_y_train, s_y_test = train_test_split(scaledX, y, test_size = .2, random_state=1)\n",
    "\n",
    "# print(s_x_train)\n",
    "# print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8528fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standalone PCA for dimension reduction\n",
    "#https://analyticsindiamag.com/principal-component-analysis-in-python/#:~:text=pca%20%3D%20PCA(n_components%20%3D%20number,explained_variance%20%3D%20pca.explained_variance_ratio_\n",
    "pca = PCA(n_components = 20)\n",
    "pca.fit(scaled_X)\n",
    "variance = pca.explained_variance_ratio_\n",
    "\n",
    "#scree plot\n",
    "PC_values = np.arange(pca.n_components_) + 1\n",
    "plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Explained')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230e84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline to search for the best combination of PCA truncation and classifier regularization.\n",
    "# https://scikit-learn.org/stable/auto_examples/compose/plot_digits_pipe.html#sphx-glr-auto-examples-compose-plot-digits-pipe-py\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "# set the tolerance to a large value to make the example faster\n",
    "logistic = LogisticRegression(max_iter=10000, tol=0.1)\n",
    "pipe = Pipeline(steps=[(\"pca\", pca), (\"logistic\", logistic)])\n",
    "\n",
    "# Set parameters of pipelines\n",
    "param_grid = {\n",
    "    \"pca__n_components\": [1, 5, 15, 30, 45, 60, 65],\n",
    "    \"logistic__C\": np.logspace(-4, 4, 4),\n",
    "}\n",
    "search = GridSearchCV(pipe, param_grid, n_jobs=2)\n",
    "search.fit(scaled_X, y)\n",
    "print(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\n",
    "print(search.best_params_)\n",
    "\n",
    "# Plot the PCA spectrum\n",
    "pca.fit(scaled_X)\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, sharex=True, figsize=(6, 6))\n",
    "ax0.plot(\n",
    "    np.arange(1, pca.n_components_ + 1), pca.explained_variance_ratio_, \"+\", linewidth=2\n",
    ")\n",
    "ax0.set_ylabel(\"PCA explained variance ratio\")\n",
    "\n",
    "ax0.axvline(\n",
    "    search.best_estimator_.named_steps[\"pca\"].n_components,\n",
    "    linestyle=\":\",\n",
    "    label=\"n_components chosen\",\n",
    ")\n",
    "ax0.legend(prop=dict(size=12))\n",
    "\n",
    "# For each number of components, find the best classifier results\n",
    "results = pd.DataFrame(search.cv_results_)\n",
    "components_col = \"param_pca__n_components\"\n",
    "best_clfs = results.groupby(components_col).apply(\n",
    "    lambda g: g.nlargest(1, \"mean_test_score\")\n",
    ")\n",
    "\n",
    "best_clfs.plot(\n",
    "    x=components_col, y=\"mean_test_score\", yerr=\"std_test_score\", legend=False, ax=ax1\n",
    ")\n",
    "ax1.set_ylabel(\"Classification accuracy (val)\")\n",
    "ax1.set_xlabel(\"n_components\")\n",
    "\n",
    "plt.xlim(-1, 70)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd0b5ce",
   "metadata": {},
   "source": [
    "## Data Preparation: Part 2\n",
    "*Describe the final dataset that is used for classification/regression (include a description of newly formed variables you created*\n",
    "\n",
    "The final dataset that will be used for classification of individuals with an income greater than 50k includes 63 predictor variables. Seven of the eight original categorical variables, including sex, marital status, relationship, race, workclass, occupation, and education, have been one-hot encoded, resulting in a larger number of predictor variables. The eighth original categorical variable, native country, has been trasnformed to a binary variable indicitive of immigrant status; records with native country as 'United States' were assigned a value of 0 while all else was assigned a value of 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21108b06",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation: Part 1\n",
    "\n",
    "*Choose and explain your evaluation metrics that you will use (i.e., accuracy, precision, recall, F-measure, or any metric we have discussed). Why are the measures appropriate for analyzing the results of your modeling? Give a detailed explanation backing up any assertions.*\n",
    "\n",
    "We will be using accuracy as our evaluation metric to compare across multiple types of models. Accuracy is a reasonable metric to use for this comparison since it portrays how well the model performs, and each model can assess for accuracy so the evaluation metric will be the same. Within each model assessment, various evaluation metrics derived from the models will be considered during the refinement process. \n",
    "\n",
    "Precision and recall may be considered depending on the needs of the client. But at this time there is not a need to determine a models performance based on the comparison of false negatives or false positives, so accuracy will suffice.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e008ad8",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation: Part 2\n",
    "\n",
    "*Choose the method you will use for diving your data into training and testing splits (i.e. are you using Stratified 10-fold cross validation? Why?). Explain why your chosen method is appropriate or use more than one method as appropriate*\n",
    "\n",
    "In preparation of classification and regression modeling, the final dataset has been given an 80/20 randomized split on the X's and the y to form the tables x_train, x_test, y_train, and y_test. The x_train table will be used to build each model, then the y_train values will be compared to the predicted values of the x_train model's to asses each's accuracy. The use of the x_test and y_test tables are to assess the accuracy of the models' predictions on a subset of data that was not used in the building of the model. This form of splitting is appropriate as our datasize is large enough. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51285dfe",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation: Part 3\n",
    "\n",
    "*Create three different classification/regression models (e.g. random forest, KNN, and SVM). Two modeling techniques must be new (but the third could be SVM or logistic regression). Adjust parameters as appropriate to increase generalization performance using your chose metric.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be608b99",
   "metadata": {},
   "source": [
    "### Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1412b085",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "lr = LogisticRegression(C=1.0, random_state=1, solver='lbfgs')\n",
    "\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "print('accuracy', mt.accuracy_score(y_test, y_pred))\n",
    "print('confusion matrix\\n', mt.confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794c62de",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = lr.coef_.T\n",
    "variable_names = df.columns\n",
    "for coef, name in zip(weights, variable_names):\n",
    "    print (name, \"has weight of\", coef[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d920bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as pyplot\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "weights = pd.Series(lr.coef_[0], index=df.columns)\n",
    "weights.plot(kind='bar')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c003c2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "plotly.offline.init_notebook_mode() # run at the start of every notebook\n",
    "\n",
    "graph1 = {'x': df.columns,\n",
    "          'y': weights,\n",
    "       'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca83ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "# Age Interpretation \n",
    "print(math.exp(-0.006799438255839271))\n",
    "# Marital_Married-civ-spouse interpretation\n",
    "print(math.exp(0.00037492480181653277))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b9160",
   "metadata": {},
   "source": [
    "### Initial Logistic Regression ### \n",
    "<p>The features of importance are notated as those furthest from 0 as this would simulate the greates changes within the model. In this initial logistic regaression model with all attributes included, the features that appear to be of importance are age, hours-per-week, capital loss, education-num, Marital_Never_married, Marital_Married-civ-spouse, Work_Private, Rel_Husband, capital-gain, and Rel_Not-in-family. For example, if the age of an individual increases by one unit, the esimated odds of having an income greater than 50k change by a factor of 0.99. In other words, the odds decrease by 1%. In another example, if a person is a married civilian spouse, the estimated odds of having an income greater than 50k is .03% higher than someone who is not a married civilian spouse. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df[['age','hours-per-week', 'capital-loss','education-num','Marital_Never-married','Marital_Married-civ-spouse','Work_Private', 'Rel_Husband','capital-gain','Rel_Not-in-family']].values\n",
    "\n",
    "sc.fit(Xnew)\n",
    "x_train, x_test, y_train, y_test = train_test_split(Xnew, y, test_size = .2, random_state=1)\n",
    "lr.fit(x_train, y_train)\n",
    "y_pred = lr.predict(x_test)\n",
    "\n",
    "print('accuracy', mt.accuracy_score(y_test, y_pred))\n",
    "print('confusion matrix\\n', mt.confusion_matrix(y_test, y_pred))\n",
    "\n",
    "\n",
    "graph1 = {'x': ['age','hours-per-week', 'capital-loss','education-num','Marital_Never-married','Marital_Married-civ-spouse','Work_Private', 'Rel_Husband','capital-gain','Rel_Not-in-family'],\n",
    "        'y': weights,\n",
    "        'type': 'bar'}\n",
    "\n",
    "fig = dict()\n",
    "fig['data'] = [graph1]\n",
    "fig['layout'] = {'title': 'Logistic Regression Weights, with error bars'}\n",
    "\n",
    "plotly.offline.iplot(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40217d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capital Loss interpretation\n",
    "math.exp(-0.001613973)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdab62",
   "metadata": {},
   "source": [
    "#### Simplified Logistic Regression Model ###\n",
    "<p>In this simplified logistic regression model, the features that appear to be the most important are age, capital-loss, and Marital_Married-civ-spouse. For an example, each one unit increase in capital loss, the odds of having an income over the age of 50k changes by 0.998. In other words, the odds decrease 0.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b8d298",
   "metadata": {},
   "source": [
    "### Model 2: SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072ddf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "train_svm = SVC(kernel = 'rbf', gamma=.1, C=1.0)\n",
    "\n",
    "train_svm.fit(x_train, y_train)\n",
    "\n",
    "y_pred = train_svm.predict(x_test)\n",
    "\n",
    "print('Accuracy: %.3f' % accuracy_score(y_test, y_pred))\n",
    "print('Classification Error: %.3f' % (1 - (accuracy_score(y_test, y_pred))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70548197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Support vecors \n",
    "train_svm.support_vectors_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4903e072",
   "metadata": {},
   "source": [
    "### Non-linear SVM Model ###\n",
    "The chosen support vectors for this model show the coordinates of the observations that are closest to the hyperplane, or decision boundary. These points influence the position and orientation of the decision boundary to maximize the accuracy of the classifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb8802",
   "metadata": {},
   "source": [
    "### Explanation ###\n",
    "<p>The initial logistic regression model with all attributes included, resulted in a prediction accuracy of 78.6%. While the speed of this model prediction was quick, a second logistic model was ran with the top 10 attributes of importance by weight. The second logistic regression model, including the atributes age, hours-per-week, capital-loss, education-num, Marital_Never-married,Marital_Married-civ-spouse, Work_Private, Rel_Husband, capital-gain, and Rel_Not-in-family, resulted in a predition accuracy of 79.9%. In addition to the prediction accuracy being better in the second logistic regression model, the speed of the model prediction is quicker as well since there are less varibles being included in the model. Finally, the non-linear SVM model, with the same attributes as the second logistic regression model, resulted in a prediction accuracy of 83.6%. The non-linear SVM model resulted in the best prediction accuracy, however the trade-off is in the increased time it takes to run this model as opposed to the logistic regression models. In terms of efficiency in prediction, the non-linear SVM model is better than the logistic regression models since the SVM model relies on the few support vectors. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c31a0b",
   "metadata": {},
   "source": [
    "### Model 3: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data frame to load in the accuracy score for each K chosen\n",
    "points = pd.DataFrame(columns=['k_score'])\n",
    "\n",
    "# Run thorugh different K neighbors and load into a dataframe\n",
    "for K in range(1,30):\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=K, weights='uniform', metric='euclidean')\n",
    "    clf_knn.fit(x_train,y_train)\n",
    "    preds = clf_knn.predict(x_test)\n",
    "    \n",
    "    points.loc[K] = [accuracy_score(y_test,preds)]\n",
    "    # print('KNN Accuracy of classifier with %d neighbors is: %.2f'%(K,acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b1d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(points)\n",
    "\n",
    "plt.plot(points.k_score)\n",
    "plt.title(\"Accuracy for K Neighbors\")\n",
    "plt.xlabel('K Neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdaa407",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_knn = KNeighborsClassifier(n_neighbors=6, weights='uniform', metric='euclidean')\n",
    "clf_knn.fit(x_train,y_train)\n",
    "preds = clf_knn.predict(x_test)\n",
    "    \n",
    "acc = accuracy_score(y_test,preds)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f3ba8e",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "\n",
    "K Nearest Neighbors classification was run using a range of k values from 1-30. The accuracy within each model were visualized by the number chosen for k. From this graph, we were able to determine that a model that classifies based on 6 nearest neighbors would provide the highest accuracy. We created a model using knn with 6 as the k value for nearest neighbors. This model resulted in an accuracy score of 83.82%. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46830194",
   "metadata": {},
   "source": [
    "### Model 4: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3635664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Model \n",
    "forest = RandomForestClassifier()\n",
    "# Fitting the Model\n",
    "forest.fit(x_train,y_train)\n",
    "# Predicting the Model\n",
    "preds = forest.predict(x_test)\n",
    "# Obtaining and printing the accuracy\n",
    "f = accuracy_score(y_test,preds)\n",
    "print(\"Random Forest Accuracy: \",f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcc10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "# Building a confusion matrix\n",
    "cm = confusion_matrix(y_test,preds)\n",
    "\n",
    "print(cm)\n",
    "\n",
    "from seaborn import heatmap\n",
    "heatmap(cm/np.sum(cm), annot=True, fmt='.2%', cmap='Reds')\n",
    "plt.title('Random Forest Prediction Accuracy Heat Map')\n",
    "\n",
    "print(classification_report(y_test,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef1f8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model \n",
    "forestTwo = RandomForestClassifier(max_depth=4)\n",
    "# Fitting the Model\n",
    "forestTwo.fit(x_train,y_train)\n",
    "# Predicting the Model\n",
    "predsTwo = forestTwo.predict(x_test)\n",
    "# Obtaining and printing the accuracy\n",
    "g = accuracy_score(y_test,predsTwo)\n",
    "print(\"Random Forest Accuracy: \",g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907b0503",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmTwo = confusion_matrix(y_test,predsTwo)\n",
    "\n",
    "print(cmTwo)\n",
    "heatmap(cmTwo/np.sum(cmTwo), annot=True, fmt='.2%', cmap='Reds')\n",
    "plt.title('Random Forest Prediction Accuracy Heat Map')\n",
    "\n",
    "print(classification_report(y_test,predsTwo))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4397ec3c",
   "metadata": {},
   "source": [
    "### Explanation \n",
    "\n",
    "A random forest model was run without any controls on how the classification would be constructed. This initial model resulted in an accuracy of 83.60%. An alternative random forest model was run, this time limiting the size of the tree to a max depth of 4. This alternative random forest model resulted in an accuracy of 84.5%. Through the visualizations of the heatmaps, we can see where the tradeoff lies. For instance, limiting the tree to have a max depth of 4 nodes, causes over a 2% loss of proportion of individuals who were correctly classified as having an income greater than 50k. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ad1d5",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation: Part 4\n",
    "*Analyze the results using your chosen method of evaluation. Use visualizations of the results to bolster the analysis. Explain any visuals and analyze why they are interesting to someone that that might use this model.*\n",
    "\n",
    "--- INSERT EXPLANATION --- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e0121d",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation: Part 5\n",
    "*Discuss the advantages of each model for each classification task, if any. If there are not advantages, explain why. Is any model better than another? Is the difference significant with 95% confidence? Use proper statistical methods.*\n",
    "\n",
    "--- INSERT EXPLANATION --- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e0c75",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation: Part 6\n",
    "*Which attributes from your analysis are more important? Use proper methods discussed in class to evaluate the importance of different attributes. Discuss the results and hypothesize about why certain attributes are more important than others for a given classification task. *\n",
    "\n",
    "--- INSERT EXPLANATION --- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9cc6bd0",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "*How useful is your model for interested parties (i.e. the companies or organizations that might want to use it for prediction)? How would you measure the model's value if it was used by these parties? How would you deploy your mocel for interested parties? What other data should be collected? How often would the model need to be updated, etc?*\n",
    "\n",
    "--- INSERT ANSWER --- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb84790",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    "\n",
    "*One idea: Grid search parameters in a parallelized fashion and visualize the performances across attributes. Which parameters are most significant for making a good model for each classification algorithm? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8562a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit all models\n",
    "clf = LazyClassifier(predictions=True)\n",
    "models, predictions = clf.fit(x_train, x_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
