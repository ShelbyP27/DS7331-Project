{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baab44a",
   "metadata": {},
   "source": [
    "# DS7331 Project 3\n",
    "#### Group 2: Hollie Gardner, Cleveland Johnson, Shelby Provost\n",
    "[Dataset Source](https://archive-beta.ics.uci.edu/ml/datasets/census+income)<br/>\n",
    "[Github Repo](https://github.com/ShelbyP27/DS7331-Project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae9caea",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329f550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/rmagic.py:11: UserWarning: The rmagic extension in IPython has moved to `rpy2.ipython`, please see `rpy2` documentation.\n",
      "  warnings.warn(\"The rmagic extension in IPython has moved to \"\n"
     ]
    }
   ],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#prediction models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# cluster analysis\n",
    "\n",
    "# arm\n",
    "# https://mhahsler.github.io/arules/docs/python/arules_python.html\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "arules = importr(\"arules\")\n",
    "arulesViz = importr(\"arulesViz\")\n",
    "\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext rmagic\n",
    "%load_ext rpy2.ipython \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481c964",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa6180",
   "metadata": {},
   "source": [
    "### Loading and Prepping Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f225805",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education</th>\n",
       "      <th>education-num</th>\n",
       "      <th>marital-status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>native-country</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>State-gov</td>\n",
       "      <td>77516</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Never-married</td>\n",
       "      <td>Adm-clerical</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>2174</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  fnlwgt  education  education-num  \\\n",
       "0   39         State-gov   77516  Bachelors             13   \n",
       "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
       "2   38           Private  215646    HS-grad              9   \n",
       "3   53           Private  234721       11th              7   \n",
       "4   28           Private  338409  Bachelors             13   \n",
       "\n",
       "       marital-status         occupation   relationship   race     sex  \\\n",
       "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
       "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
       "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
       "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
       "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
       "\n",
       "   capital-gain  capital-loss  hours-per-week native-country income  \n",
       "0          2174             0              40  United-States  <=50K  \n",
       "1             0             0              13  United-States  <=50K  \n",
       "2             0             0              40  United-States  <=50K  \n",
       "3             0             0              40  United-States  <=50K  \n",
       "4             0             0              40           Cuba  <=50K  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the census dataset using pandas\n",
    "# Reading the CSV file after converting file to csv and removing superfluous spaces via Excel.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ShelbyP27/DS7331-Project/main/adult-data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1faf4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up dataset by removing na values & creating one-hot response\n",
    "df = df.replace(to_replace='?',value=np.nan) # replace '?' with NaN (not a number)\n",
    "df.dropna(inplace=True) # Removing na values\n",
    "df.duplicated(subset=None, keep='first') #Remove duplicates\n",
    "\n",
    "#change weird value for income\n",
    "df['income'] = df['income'].replace(['<=50K'],'under50K')\n",
    "df['income'] = df['income'].replace(['>50K'],'over50K')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "549c4167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove attributes\n",
    "del df['education-num']\n",
    "del df['fnlwgt']\n",
    "del df['capital-gain']\n",
    "del df['capital-loss']\n",
    "\n",
    "#create categorical groups out of continuous\n",
    "#change age to age_group\n",
    "age_group = pd.cut(df.age,bins=[12,19,34,65,120], labels=['teenager','early adult','adult','older adult'])\n",
    "df.insert(2,'agegroup',age_group)\n",
    "del df['age']\n",
    "\n",
    "#change hours-per-week to \n",
    "hours_group = pd.cut(df['hours-per-week'], bins=[0,30,40,168], labels=['part-time','full-time','over-time'])\n",
    "df.insert(9,'hoursgroup',hours_group)\n",
    "del df['hours-per-week']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba164d",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "*Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific dataset and the stakeholders needs?*\n",
    "\n",
    "The purpose of the dataset selected is to identify what characteristics an individual needs to possess in order to make an income exceeding $50,000. Since we've chosen to analyze using association rule mining, then measurements of effectiveness will be lift and [insert here] when we filter to our question of interest of whether income is greater or less than $50k is on the right hand side. The algorithm must also not be a burden computational, therefore we will adjust our confidence and support requirements. The length of itemlists on the left-hand-side will ideally be greater than 1. \n",
    "\n",
    "We have chosen to use the association rule mining by treating each individual and their characteristics as a transaction. The \"item lists\", or characteristics of the individuals within this dataset, will hopefully accomplish the task of understanding what are the most important characteristics that determine whether someone in the 1990s United States would have had an income that exceed $50k. \n",
    "\n",
    "Knowing this information could assist in targeted support for families that had less than $50k and figuring out what interventions could be used to bring families out of poverty levels and into liveable income. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15d4d5",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "### Part One\n",
    "*Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?*\n",
    "\n",
    "The final dataset that will be used for the association rule minging of individuals with an income greater than 50k includes the following 10 attributes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d72bea51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 30162 entries, 0 to 32560\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   workclass       30162 non-null  object  \n",
      " 1   agegroup        30162 non-null  category\n",
      " 2   education       30162 non-null  object  \n",
      " 3   marital-status  30162 non-null  object  \n",
      " 4   occupation      30162 non-null  object  \n",
      " 5   relationship    30162 non-null  object  \n",
      " 6   race            30162 non-null  object  \n",
      " 7   sex             30162 non-null  object  \n",
      " 8   hoursgroup      30162 non-null  category\n",
      " 9   native-country  30162 non-null  object  \n",
      " 10  income          30162 non-null  object  \n",
      "dtypes: category(2), object(9)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6311fa62",
   "metadata": {},
   "source": [
    "All of the continuous variables have either been removed or converted into categorical groups for easier identification in the transaction lists. \n",
    "\n",
    "According to the Kohavi and Becker, the following changes have already been made to the raw data:\n",
    "* Discretized agrossincome into two ranges with threshold 50,000.\n",
    "* Convert U.S. to US to avoid periods.\n",
    "* Convert Unknown to \"?\"\n",
    "* Run MLC++ GenCVFiles to generate data,test.\n",
    "<br>\n",
    "\n",
    "Therefore, we began by looking for the '?' values in the dataset by counting the number in each column. Then, we replaced as null using numpy. We decided to handle missing values in the following three variables which possessed missing values as below: \n",
    "\n",
    "* Workclass: 1836 rows\n",
    "* Occupation: 1843 rows\n",
    "* Native-Country: 583 rows\n",
    "\n",
    "There was not a way to impute values for these categorical variables and leaving as unknown variables does not add value to our analysis. Therefore, these rows were removed leaving us with 30,162 complete rows in this dataframe. As for duplicates, Kohavi and Becker described 6 duplicate rows. After reviewing our current dataframe, there do not appear to be any instances of duplicate data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc39b84e",
   "metadata": {},
   "source": [
    "### Part Two\n",
    "*Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.*\n",
    "\n",
    "----INSERT REVIEW OF ATTRIBUTE SELECTION PROCESS. MAY NEED TO REVISIT #1 & #2----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb422b",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "*Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results.*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424b790",
   "metadata": {},
   "source": [
    "### Option A: Cluster Analysis\n",
    "\n",
    " - Perform cluster analysis using several clustering methods\n",
    " - How did you determine a suitable number of clusters for each method?\n",
    " - Use internal and/or external validation measures to describe and compare the clusterings and the clusters (some visual methods would be good).\n",
    " - Describe your results. What findings are the most interesting and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32c087",
   "metadata": {},
   "source": [
    "#### Data prep for cluster analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eda3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the features from the response\n",
    "if 'income' in df:\n",
    "    y = df['income'].values\n",
    "    del df['income']\n",
    "    X = df.values\n",
    "\n",
    "# Train / Test split with scaled_X\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_X, y, test_size = .2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a reduced version of dataframe with fewer columns\n",
    "df_reduced = df[['age','hours-per-week', 'capital-loss','education-num','Marital_Never-married','Marital_Married-civ-spouse','Work_Private', 'Rel_Husband','capital-gain','Rel_Not-in-family']].values\n",
    "\n",
    "df_red_scaled = StandardScaler().fit_transform(df_reduced)\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_red_scaled, y, test_size = .2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c2376a",
   "metadata": {},
   "source": [
    "#### Kmeans Cluster Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05a5f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kmeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "X1 = df_red_scaled\n",
    "\n",
    "# run kmeans algorithm (this is the most traditional use of k-means)\n",
    "kmeans = KMeans(init='random', # initialization\n",
    "        n_clusters=100,  # number of clusters\n",
    "        n_init=5)       # number of different times to run k-means\n",
    "       \n",
    "kmeans.fit(X1)\n",
    "\n",
    "# visualize the data\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'r.', markersize=2) #plot the data\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='+', s=200, linewidths=3, color='k')  # plot the centroids\n",
    "plt.title('K-means clustering for X1')\n",
    "plt.xlabel('X1, Feature 1')\n",
    "plt.ylabel('X1, Feature 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec41f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "kmeans_mini = MiniBatchKMeans(n_clusters=2, batch_size=1000)\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "print('Time for BatchKMeans:')\n",
    "%time kmeans.fit(X1)\n",
    "print('Time for MiniBatchKMeans:')\n",
    "%time kmeans_mini.fit(X1)\n",
    "\n",
    "\n",
    "# visualize the data\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.plot(X1[:, 0], X1[:, 1], 'r.', markersize=2) #plot the data\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='+', s=200, linewidths=3, label='Batch')  # plot the centroids\n",
    "\n",
    "centroids = kmeans_mini.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='+', s=200, linewidths=3, color='k',label='Mini-Batch')  # plot the centroids\n",
    "plt.legend()\n",
    "plt.title('K-means clustering for X1')\n",
    "plt.xlabel('X1, Feature 1')\n",
    "plt.ylabel('X1, Feature 2')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0f43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# now plot each dataset\n",
    "plt.figure(figsize=(15,5))\n",
    "for i,X in enumerate([X1]):\n",
    "    plt.subplot(1,2,i+1)\n",
    "    plt.plot(X[:, 0], X[:, 1], 'r.', markersize=2) #plot the data\n",
    "    plt.title('Variable name: X{0}'.format(i+2))\n",
    "    plt.xlabel('X{0}, Feature 1'.format(i+2))\n",
    "    plt.ylabel('X{0}, Feature 2'.format(i+2))\n",
    "    plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51984ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets first look at the connectivity of the graphs and distance to the nearest neighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "X2 = X1\n",
    "\n",
    "#=======================================================\n",
    "# CHANGE THESE VALUES TO ADJUST MINPTS FOR EACH DATASET\n",
    "X2_N = 5000\n",
    "\n",
    "#=======================================================\n",
    "\n",
    "# create connectivity graphs before calcualting the hierarchy\n",
    "X2_knn_graph = kneighbors_graph(X2, X2_N, mode='distance') # calculate distance to four nearest neighbors \n",
    "\n",
    "N2 = X2_knn_graph.shape[0]\n",
    "X2_4nn_distances = np.zeros((N2,1))\n",
    "for i in range(N2):\n",
    "    X2_4nn_distances[i] = X2_knn_graph[i,:].max()\n",
    "\n",
    "X2_4nn_distances = np.sort(X2_4nn_distances, axis=0)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(range(N2), X2_4nn_distances, 'r.', markersize=2) #plot the data\n",
    "plt.title('Dataset name: X2, sorted by neighbor distance')\n",
    "plt.xlabel('X2, Instance Number')\n",
    "plt.ylabel('X2, Distance to {0}th nearest neighbor'.format(X2_N))\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a771cda",
   "metadata": {},
   "source": [
    "### Option B: Association Rule Mining\n",
    " - Create frequent itemsets and association rules.\n",
    " - Use tables/visualization to discuss the found results.\n",
    " - Use several measure for evaluating how interesting different rules are.\n",
    " - Describe your results. What findings are the most compelling and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11411f41",
   "metadata": {},
   "source": [
    "### Step One\n",
    "We begin with a generic support set at .5 and confidence at .8. The maxlen is set at 5 to reduce computational load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5aaaabda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori\n",
      "\n",
      "Parameter specification:\n",
      " confidence minval smax arem  aval originalSupport maxtime support minlen\n",
      "        0.8    0.1    1 none FALSE            TRUE       5     0.5      2\n",
      " maxlen target  ext\n",
      "      5  rules TRUE\n",
      "\n",
      "Algorithmic control:\n",
      " filter tree heap memopt load sort verbose\n",
      "    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n",
      "\n",
      "Absolute minimum support count: 15081 \n",
      "\n",
      "set item appearances ...[0 item(s)] done [0.00s].\n",
      "set transactions ...[107 item(s), 30162 transaction(s)] done [0.05s].\n",
      "sorting and recoding items ... [7 item(s)] done [0.00s].\n",
      "creating transaction tree ... done [0.02s].\n",
      "checking subsets of size 1 2 3 done [0.00s].\n",
      "writing ... [15 rule(s)] done [0.00s].\n",
      "creating S4 object  ... done [0.01s].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<rpy2.robjects.methods.RS4 object at 0x12aa23640> [RTYPES.S4SXP]\n",
       "R classes: ('rules',)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run apriori on dataframe\n",
    "%R -i df rules <- apriori(df,parameter = list(minlen=2, maxlen=5, supp=0.5, conf=0.8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c93d7f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     lhs                                rhs                              support confidence  coverage      lift count\n",
      "[1]  {sex=Male,                                                                                                      \n",
      "      native-country=United-States}  => {race=White}                   0.5585836  0.9071721 0.6157417 1.0551083 16848\n",
      "[2]  {sex=Male}                      => {race=White}                   0.5980373  0.8850834 0.6756846 1.0294176 18038\n",
      "[3]  {race=White,                                                                                                    \n",
      "      sex=Male}                      => {native-country=United-States} 0.5585836  0.9340282 0.5980373 1.0242931 16848\n",
      "[4]  {race=White}                    => {native-country=United-States} 0.8029308  0.9338680 0.8597905 1.0241175 24218\n",
      "[5]  {native-country=United-States}  => {race=White}                   0.8029308  0.8805265 0.9118759 1.0241175 24218\n",
      "[6]  {workclass=Private,                                                                                             \n",
      "      native-country=United-States}  => {race=White}                   0.5877594  0.8804569 0.6675618 1.0240366 17728\n",
      "[7]  {workclass=Private,                                                                                             \n",
      "      race=White}                    => {native-country=United-States} 0.5877594  0.9263730 0.6344738 1.0158981 17728\n",
      "[8]  {race=White,                                                                                                    \n",
      "      income=under50K}               => {native-country=United-States} 0.5861349  0.9258930 0.6330482 1.0153717 17679\n",
      "[9]  {native-country=United-States,                                                                                  \n",
      "      income=under50K}               => {race=White}                   0.5861349  0.8620118 0.6799615 1.0025836 17679\n",
      "[10] {sex=Male}                      => {native-country=United-States} 0.6157417  0.9112856 0.6756846 0.9993527 18572\n"
     ]
    }
   ],
   "source": [
    "#looking at the top rules\n",
    "%R inspect(head(rules, n = 10, by = \"lift\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224525d",
   "metadata": {},
   "source": [
    "We see the top 10 rules ordered by the highest lift. These rules seem to highlight the large number of Males from the United States who are represented in this dataset. \n",
    "\n",
    "Next, we will limit to rules that have income attribute representing all levels within the right-hand-set (rhs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9354a2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keeps getting a recursion error\n",
    "#%R rules.sorted <- sort(rules, by='lift')\n",
    "#%R plot(rules.sorted, method='grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "08befd2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori\n",
      "\n",
      "Parameter specification:\n",
      " confidence minval smax arem  aval originalSupport maxtime support minlen\n",
      "        0.5    0.1    1 none FALSE            TRUE       5    0.05      1\n",
      " maxlen target  ext\n",
      "     10  rules TRUE\n",
      "\n",
      "Algorithmic control:\n",
      " filter tree heap memopt load sort verbose\n",
      "    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n",
      "\n",
      "Absolute minimum support count: 1508 \n",
      "\n",
      "set item appearances ...[2 item(s)] done [0.01s].\n",
      "set transactions ...[2 item(s), 30162 transaction(s)] done [0.11s].\n",
      "sorting and recoding items ... [2 item(s)] done [0.00s].\n",
      "creating transaction tree ... done [0.01s].\n",
      "checking subsets of size 1 done [0.00s].\n",
      "writing ... [0 rule(s)] done [0.01s].\n",
      "creating S4 object  ... done [0.02s].\n"
     ]
    }
   ],
   "source": [
    "## find only itemsets with income attribute in the right-hand-side.\n",
    "# from https://cran.r-project.org/web/packages/arules/arules.pdf \n",
    "%R rules <- apriori(df, parameter = list(minlen=1, maxlen=10, supp=0.05, conf=0.5), appearance = list(items = c(\"income=over50K\", \"income=under50K\")))\n",
    "%R inspect(head(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23cb21df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the top rules\n",
    "%R inspect(head(rules, n = 10, by = \"lift\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315d02f",
   "metadata": {},
   "source": [
    "## Deployment\n",
    " - Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling?\n",
    " - How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)?\n",
    " - How would your deploy your model for interested parties?\n",
    " - What other data should be collected?\n",
    " - How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f79b8f",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    " - You have free reign to provide additional analyses or combine analyses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
