{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baab44a",
   "metadata": {},
   "source": [
    "# DS7331 Project 3\n",
    "#### Group 2: Hollie Gardner, Cleveland Johnson, Shelby Provost\n",
    "[Dataset Source](https://archive-beta.ics.uci.edu/ml/datasets/census+income)<br/>\n",
    "[Github Repo](https://github.com/ShelbyP27/DS7331-Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "329f550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "#import lazypredict\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pca and gridsearch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#prediction models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#exceptional work (working on this)\n",
    "import math \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea39c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from michael hahsler's github -- https://mhahsler.github.io/arules/docs/python/arules_python.html\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "arules = importr(\"arules\")\n",
    "\n",
    "# some helper functions\n",
    "def arules_as_matrix(x, what = \"items\"):\n",
    "    return ro.r('function(x) as(' + what + '(x), \"matrix\")')(x)\n",
    "\n",
    "def arules_as_dict(x, what = \"items\"):\n",
    "    l = ro.r('function(x) as(' + what + '(x), \"list\")')(x)\n",
    "    l.names = [*range(0, len(l))]\n",
    "    return dict(zip(l.names, map(list,list(l))))\n",
    "\n",
    "def arules_quality(x):\n",
    "    return x.slots[\"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894f6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/rmagic.py:11: UserWarning: The rmagic extension in IPython has moved to `rpy2.ipython`, please see `rpy2` documentation.\n",
      "  warnings.warn(\"The rmagic extension in IPython has moved to \"\n"
     ]
    }
   ],
   "source": [
    "#association rules\n",
    "import time\n",
    "%matplotlib inline\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2 import robjects as robj\n",
    "\n",
    "%load_ext rmagic\n",
    "%load_ext rpy2.ipython \n",
    "# this enables the %R extension to iPython (does not work outside of the iPython shell)\n",
    "\n",
    "# these packages will need to be installed\n",
    "# open R and run \n",
    "#     install.package(arules)\n",
    "#     install.package(arulesViz)\n",
    "#lib='C:/Users/johnc45/R/R-Library'\n",
    "#arules = importr('arules', lib_loc=lib) # same as importing in R with the \"library\" command\n",
    "#arules_viz = importr('arulesViz', lib_loc=lib) # visualize the different rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481c964",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa6180",
   "metadata": {},
   "source": [
    "### Loading and Prepping Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f225805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the census dataset using pandas\n",
    "# Reading the CSV file after converting file to csv and removing superfluous spaces via Excel.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ShelbyP27/DS7331-Project/main/adult-data.csv')\n",
    "\n",
    "# Getting a first look at the dataset\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1faf4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up data set\n",
    "df = df.replace(to_replace='?',value=np.nan) # replace '?' with NaN (not a number)\n",
    "df.dropna(inplace=True) # Removing na values\n",
    "df.duplicated(subset=None, keep='first') #Remove duplicates\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int) #One-hot respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff4a638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encoding the Categorical variables\n",
    "if 'sex' in df:\n",
    "    df['IsMale'] = df.sex == 'Male'\n",
    "    df.IsMale = df.IsMale.astype(np.int64)\n",
    "    del df['sex']\n",
    "    \n",
    "if 'marital-status' in df:\n",
    "    tmp_df = pd.get_dummies(df['marital-status'], prefix = 'Marital')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['marital-status']\n",
    "    \n",
    "if'relationship' in df:\n",
    "    tmp_df = pd.get_dummies(df['relationship'], prefix = 'Rel')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['relationship']\n",
    "\n",
    "if 'race' in df:\n",
    "    tmp_df = pd.get_dummies(df['race'], prefix = 'Race')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['race']\n",
    "\n",
    "if 'workclass' in df:\n",
    "    tmp_df = pd.get_dummies(df['workclass'], prefix = 'Work')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['workclass']\n",
    "\n",
    "if 'occupation' in df:\n",
    "    tmp_df = pd.get_dummies(df['occupation'], prefix = 'Occupation')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['occupation']\n",
    "\n",
    "if 'education' in df:\n",
    "    tmp_df = pd.get_dummies(df['education'], prefix = 'Education')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['education']\n",
    "\n",
    "    \n",
    "#Replace Native Country with Immigrant atribute\n",
    "if 'native-country' in df:\n",
    "    df['immigrant'] = np.where(df['native-country']!= 'United-States', 1, 0)\n",
    "    del df['native-country']\n",
    "\n",
    "df2 = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31eda3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the response\n",
    "if 'income' in df:\n",
    "    y = df['income'].values\n",
    "    del df['income']\n",
    "    X = df.values\n",
    "\n",
    "# Train / Test split with scaled_X\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_X, y, test_size = .2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109b08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df[['age','hours-per-week', 'capital-loss','education-num','Marital_Never-married','Marital_Married-civ-spouse','Work_Private', 'Rel_Husband','capital-gain','Rel_Not-in-family']].values\n",
    "\n",
    "newscaled_X = StandardScaler().fit_transform(Xnew)\n",
    "x_train, x_test, y_train, y_test = train_test_split(newscaled_X, y, test_size = .2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba164d",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "*Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific dataset and the stakeholders needs?*\n",
    "\n",
    "--- INSERT EXPLANATION----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15d4d5",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "*Part One: Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?*\n",
    "\n",
    "---- INSERT REVIEW OF ABOVE DATA PREP PROCESS----\n",
    "\n",
    "*Part Two: Visualize the any important attributes appropriately. Important: Provide an interpretation for any charts or graphs.*\n",
    "\n",
    "----INSERT REVIEW OF ATTRIBUTE SELECTION PROCESS. MAY NEED TO REVISIT #1 & #2----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb422b",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "*Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results.*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424b790",
   "metadata": {},
   "source": [
    "### Option A: Cluster Analysis\n",
    "\n",
    " - Perform cluster analysis using several clustering methods\n",
    " - How did you determine a suitable number of clusters for each method?\n",
    " - Use internal and/or external validation measures to describe and compare the clusterings and the clusters (some visual methods would be good).\n",
    " - Describe your results. What findings are the most interesting and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a771cda",
   "metadata": {},
   "source": [
    "### Option B: Association Rule Mining\n",
    " - Create frequent itemsets and association rules.\n",
    " - Use tables/visualization to discuss the found results.\n",
    " - Use several measure for evaluating how interesting different rules are.\n",
    " - Describe your results. What findings are the most compelling and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a88de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori\n",
      "\n",
      "Parameter specification:\n",
      " confidence minval smax arem  aval originalSupport maxtime support minlen\n",
      "        0.8    0.1    1 none FALSE            TRUE       5    0.05      2\n",
      " maxlen target  ext\n",
      "     10  rules TRUE\n",
      "\n",
      "Algorithmic control:\n",
      " filter tree heap memopt load sort verbose\n",
      "    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n",
      "\n",
      "Absolute minimum support count: 1508 \n",
      "\n",
      "set item appearances ...[0 item(s)] done [0.00s].\n",
      "set transactions ...[70 item(s), 30162 transaction(s)] done [0.20s].\n",
      "sorting and recoding items ... [70 item(s)] done [0.02s].\n",
      "creating transaction tree ... done [0.04s].\n",
      "checking subsets of size 1 2 3 4 5 done [7.76s].\n",
      "writing ... [51982494 rule(s)] done [27.52s].\n",
      "creating S4 object  ... done [26.18s].\n"
     ]
    }
   ],
   "source": [
    "%R -i df rules <- apriori(df,parameter = list(minlen=2, supp=0.05, conf=0.8))\n",
    "%R rules.sorted <- sort(rules, by='lift')\n",
    "%R plot(rules.sorted, method='grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6690cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o df_from_R df_from_R<-df\n",
    "\n",
    "# now we have the exact same dataset as the one from R\n",
    "# but it is now a pandas dataframe\n",
    "print(df_from_R.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb73cd7",
   "metadata": {},
   "source": [
    "### Option C: Collaborative Filtering\n",
    " - Create user-item matrices or item-item matrices using collaborative filtering\n",
    " - Determine performance of the recommendations using different performance measures and explain what each measure\n",
    " - Use tables/visualization to discuss the found results. Explain each visualization in detail.\n",
    " - Describe your results. What findings are the most compelling and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315d02f",
   "metadata": {},
   "source": [
    "## Deployment\n",
    " - Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling?\n",
    " - How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)?\n",
    " - How would your deploy your model for interested parties?\n",
    " - What other data should be collected?\n",
    " - How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f79b8f",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    " - You have free reign to provide additional analyses or combine analyses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
